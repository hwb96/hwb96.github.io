<!doctype html>

<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>奖励系统的种类 - Han Wenbo</title>
  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="description" content="The HTML5 Herald" />
<meta name="author" content="HanWenbo" /><meta property="og:url" content="http://localhost:1313/posts/20250216-reward-models-types-and-applications-in-llm-training/">
  <meta property="og:site_name" content="Han Wenbo">
  <meta property="og:title" content="奖励系统的种类">
  <meta property="og:description" content="在RewardBench中，分了Seq. Classifier，Custom Classifiers，DPO，Random，Generative5种类型的奖励模型，我比较知道的是Sequence Classifiers 和Generative，今天也主要记录这两者。">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-16T16:47:22+08:00">
    <meta property="article:modified_time" content="2025-02-16T16:47:22+08:00">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="奖励系统的种类">
  <meta name="twitter:description" content="在RewardBench中，分了Seq. Classifier，Custom Classifiers，DPO，Random，Generative5种类型的奖励模型，我比较知道的是Sequence Classifiers 和Generative，今天也主要记录这两者。">

<meta name="generator" content="Hugo 0.148.2">
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="http://localhost:1313/js/mathjax-config.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>

  <link rel="stylesheet" href="http://localhost:1313/css/normalize.min.css" />
  <link rel="stylesheet" href="http://localhost:1313/fontawesome/css/all.min.css" />
  
    
    <link href="//fonts.googleapis.com/css?family=Playfair Display:400,700|PT Serif:400,700|Merriweather:400,700" rel="stylesheet">
  
  
  <link rel="stylesheet" type="text/css" href="http://localhost:1313/css/styles.css" /><link rel='stylesheet' href='http://localhost:1313/css/custom.css'>
</head>

<body>
  <div id="container">
    <header>
      
      <h1>
        <a href="http://localhost:1313/">Han Wenbo</a>
      </h1>

      <ul id="social-media">
             <li>
               <a href="https://github.com/hwb96" title="GitHub">
               <i class="fab fa-github fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://twitter.com/realhanwenbo" title="Twitter">
               <i class="fab fa-twitter fa-lg"></i>
               </a>
             </li>
      </ul>
      
      <p><em>Focus on technology, trends, and their intertwined politics.</em></p>
      
    </header>

    
<nav>
    <ul>
        
        <li>
            <a class="active" href="http://localhost:1313/posts/">
                <i class="fa-li fa  fa-lg"></i><span>Posts</span>
            </a>
        </li>
        
        <li>
            <a class="" href="http://localhost:1313/tags">
                <i class="fa-li fa  fa-lg"></i><span>Tags</span>
            </a>
        </li>
        
        <li>
            <a class="" href="http://localhost:1313/about/about-me/">
                <i class="fa-li fa  fa-lg"></i><span>About</span>
            </a>
        </li>
        
    </ul>
</nav>


    <main>




<article>

    <h1>奖励系统的种类</h1>

    
      <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2025-02-16T16:47:22&#43;08:00">Feb 16, 2025</time>
        </li>
        
        

        

        <li>3 minute read</li>
    </ul>
</aside>

    

    
      
<div class="featured_image">
    <a href="http://localhost:1313/posts/20250216-reward-models-types-and-applications-in-llm-training/" title="奖励系统的种类">
        <img src="">
    </a>
</div>


    

    <p>在RewardBench中，分了Seq. Classifier，Custom Classifiers，DPO，Random，Generative5种类型的奖励模型，我比较知道的是Sequence Classifiers 和Generative，今天也主要记录这两者。</p>
<hr>
<p>update:重新阅读了下R1的论文，好像没有用到过程奖励模型(PRM)，以下是原文：</p>
<p><em>We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.</em></p>
<h2 id="奖励模型">奖励模型</h2>
<p><strong>DPO</strong>我之前只是之前制作偏好数据集进行DPO训练的程度，而不是利用DPO训练一个奖励模型。</p>
<p><img src="https://xtuner.readthedocs.io/zh-cn/latest/_images/preference_data.png" alt="img"></p>
<h3 id="sequence-classifiers">Sequence Classifiers</h3>
<p>我的理解是,拿Qwen模型举例，去掉[Qwen2ForCausalLM](<a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modeling_qwen2.py#766">transformers/src/transformers/models/qwen2/modeling_qwen2.py at main · huggingface/transformers</a>)中使用的vocab_size分类，使用Qwen2ForSequenceClassification的分类，这个分类是一个单分类num_labels=1。</p>
<h3 id="生成式奖励模型-grm">生成式奖励模型 (GRM)</h3>
<p>生成式奖励模型可以看看这<a href="https://papers.cool/arxiv/2408.15240,2410.12832">两篇</a>，获得奖励分数是通过直接prompt构造的。</p>
<p>在R1论文里生成80万条数据时，2.3.3说道使用了DeepSeek-V3作为generative reward model：</p>
<p><em>However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.</em></p>
<p>在r1的强化学习中同样用到了GRM：</p>
<p><em>For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts.</em></p>
<p>训练生成式奖励模型的数据集回答就是两条，一个是chosen，一个是rejected。</p>
<p>训练出来的GRM至少可以做两件事，拿<a href="https://huggingface.co/Skywork/Skywork-Critic-Llama-3.1-70B"> Skywork-Critic-Llama-3.1-70B</a>举例：</p>
<h4 id="用作偏好数据选择器">用作偏好数据选择器：</h4>
<p>回答A和回答B选择更好的一个的prompt：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#0a3069">&#34;&#34;&#34;Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user</span><span style="color:#0a3069">\&#39;</span><span style="color:#0a3069">s instructions and answers the user</span><span style="color:#0a3069">\&#39;</span><span style="color:#0a3069">s question better. 
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. 
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">Please directly output your final verdict by strictly following this format: &#34;[[A]]&#34; if assistant A is better, &#34;[[B]]&#34; if assistant B is better.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[User Question]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069"></span><span style="color:#0a3069">{input}</span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[The Start of Assistant A&#39;s Answer]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069"></span><span style="color:#0a3069">{response_a}</span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[The End of Assistant A&#39;s Answer]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[The Start of Assistant B&#39;s Answer]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069"></span><span style="color:#0a3069">{response_b}</span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[The End of Assistant B&#39;s Answer]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">&#34;&#34;&#34;</span>
</span></span></code></pre></div><h4 id="用作打分judge">用作打分Judge：</h4>
<p>prompt：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span> <span style="color:#0a3069">&#34;&#34;&#34;请扮演一个专业的对话质量评价专家，能够从多个维度诊断和分析 AI 助手对用户问题的回答，并进行总体打分（分值范围是 1-5）。你的评估应考虑回答的有用性、相关性、准确性、深度、创造性、安全性等众多维度，请注意，不同任务类型的指令对评估分析维度的侧重不一样，需要根据具体的问题进行分析。
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[用户问题]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069"></span><span style="color:#0a3069">{input}</span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[助手回答的开始]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069"></span><span style="color:#0a3069">{response_a}</span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">[助手回答的结束]
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">你的详细评估和总体打分为: &#34;&#34;&#34;</span>
</span></span></code></pre></div><p>返回：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span>**评估分析：**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">1.</span> **有用性**：回答直接解决了用户的问题，提供了清晰的步骤和计算过程，非常有用。
</span></span><span style="display:flex;"><span><span style="color:#cf222e">2.</span> **相关性**：回答完全相关于用户的问题，没有偏离主题。
</span></span><span style="display:flex;"><span><span style="color:#cf222e">3.</span> **准确性**：计算过程准确无误，结果正确。
</span></span><span style="display:flex;"><span><span style="color:#cf222e">4.</span> **深度**：回答提供了详细的步骤和解释，帮助用户理解问题的解决过程。
</span></span><span style="display:flex;"><span><span style="color:#cf222e">5.</span> **创造性**：虽然回答是直接的计算过程，但在解释每一步时使用了简单的语言和明确的计算，使得回答易于理解。
</span></span><span style="display:flex;"><span><span style="color:#cf222e">6.</span> **安全性**：回答中没有包含任何可能引起误解或危险的信息。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>**总体打分：5**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>**总结**：这个回答非常清晰、准确且有用，完全满足了用户的需求。它通过逐步解释和计算，帮助用户理解问题的解决过程。没有发现任何需要改进的地方，因此给予满分。
</span></span></code></pre></div><h2 id="基于规则的奖励系统">基于规则的奖励系统</h2>
<p>这块就和奖励模型不同了，我能想到的：r1里的格式奖励，基于代码沙盒的运行结果准确性奖励。</p>
<p>困惑度奖励，重复性奖励，长度奖励，中文字符奖励，&hellip;</p>
<p><a href="%5Bahxt/mini-r1-zero%5D(https://github.com/ahxt/mini-r1-zero)">这个项目</a>可以很好作为参考。</p>
<h2 id="rewardbench5种类型">RewardBench5种类型</h2>
<ol>
<li>
<p><strong>Sequence Classifiers</strong> (Seq. Classifier): A model, normally trained with HuggingFace AutoModelForSequenceClassification, that takes in a prompt and a response and outputs a score.</p>
</li>
<li>
<p><strong>Custom Classifiers</strong>: Research models with different architectures and training objectives to either take in two inputs at once or generate scores differently (e.g. PairRM and Stanford SteamSHP).</p>
</li>
<li>
<p><strong>DPO</strong>: Models trained with Direct Preference Optimization (DPO), with modifiers such as -ref-free or -norm changing how scores are computed. <em>Note</em>: This also includes other models trained with implicit rewards, such as those trained with <a href="https://arxiv.org/abs/2402.01306">KTO</a>.</p>
</li>
<li>
<p><strong>Random</strong>: Random choice baseline.</p>
</li>
<li>
<p><strong>Generative</strong>: Prompting fine-tuned models to choose between two answers, similar to MT Bench and AlpacaEval.</p>
</li>
</ol>
<h2 id="扩展资料">扩展资料</h2>
<p><a href="https://mp.weixin.qq.com/s/15zzzDJsFap8mFvofbEtgQ">https://mp.weixin.qq.com/s/15zzzDJsFap8mFvofbEtgQ</a></p>
<p><a href="https://papers.cool/arxiv/2408.15240,2410.12832">2408.15240, 2410.12832 | Cool Papers - Immersive Paper Discovery</a></p>
<p><a href="https://huggingface.co/spaces/allenai/reward-bench">Reward Bench Leaderboard - a Hugging Face Space by allenai</a></p>
<p><a href="https://huggingface.co/Skywork/Skywork-Critic-Llama-3.1-70B">Skywork/Skywork-Critic-Llama-3.1-70B · Hugging Face</a></p>
<p><a href="https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.1/viewer?row=4">Skywork/Skywork-Reward-Preference-80K-v0.1 · Datasets at Hugging Face</a></p>
<p><a href="https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.1/viewer?row=4">Skywork/Skywork-Reward-Preference-80K-v0.1 · Datasets at Hugging Face</a></p>
<p><a href="https://arxiv.org/pdf/2408.02666">2408.02666</a></p>
<p><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/chat/hf_engine.py#L398">LLaMA-Factory/src/llamafactory/chat/hf_engine.py at main · hiyouga/LLaMA-Factory</a></p>
<p><a href="https://arxiv.org/abs/2403.13787">RewardBench: Evaluating Reward Models for Language Modeling</a></p>

</article>


<section class="post-nav">
    <ul>
        <li>
        
            <a href="http://localhost:1313/posts/20250210-deepseek-r1-chat-template-change-analysis/"><i class="fa fa-chevron-circle-left"></i> R1 系列模型 Chat Template 变更分析与下游解决方案</a>
        
        </li>
        <li>
        
            <a href="http://localhost:1313/posts/20250808-chatgpt-5-system-prompt-zh-cn/">ChatGPT5 系统提示词 <i class="fa fa-chevron-circle-right"></i> </a>
        
        </li>
    </ul>
</section>
  
    
    
  





</main>
    <footer>
        <ul>
            <li>
                <h6>
                    Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
                    <a href="http://localhost:1313/index.xml">Subscribe </a></h6>
            </li>
            
            
        </ul>
    </footer>
</div>
<script src="http://localhost:1313/js/scripts.js"></script>

  


</body>

</html>

